{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AD2122-SentimentAnalysis-Aula8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n70YUcaCt4iJ"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "-2YeQQl7uZcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer o download do dataset IMDB\n",
        "# Este dataset está disponível nos Keras datasets: https://keras.io/api/datasets/imdb/\n",
        "# A informação está completamente processada e pronta a usar pelo classificador\n",
        "\n",
        "# O Dataset em bruto pode ser obtido aqui: https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "# Neste último caso seria necessário aplicar pré-processamento de texto antes de criar o classificador\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Estes 2 parâmetros ajudam a definir a dimensão dos exemplos lidos \n",
        "max_features = 10000\n",
        "common_words = 10\n",
        "\n",
        "# Obter o dataset, dividindo-o \n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features, skip_top=common_words)\n",
        "\n",
        "# Obter um dicionário que vai permitir descodificar as reviews\n",
        "word_index = keras.datasets.imdb.get_word_index()"
      ],
      "metadata": {
        "id": "p5Gysq_JvTq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a dimensão dos conjuntos de treino e de teste\n",
        "\n",
        "print('Train: ', x_train.shape)\n",
        "print('Test: ', x_test.shape)"
      ],
      "metadata": {
        "id": "4YeUE8Ifxb7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar algumas reviews. Altere o valor da variável review para aceder a outro texto\n",
        "\n",
        "# O dataset já vem tratado e cada exemplo é um array numpy de inteiros, em que cada inteiro representa uma palavra\n",
        "# A pontuação já foi removida, as palavras foram separadas por espaços e todos os caracteres foram convertidos para minúscula\n",
        "# Os inteiros estão organizados por frequência, logo os menores inteiros correspondem a palavras mais frequentes\n",
        "# Os inteiros 0, 1 e 2 tem um significado especial: padding, sos (starting of sequence), unk (unknown)\n",
        "\n",
        "# O código mostra também a review descodificada, \n",
        "# uma vez que o método get_word_index() obtém um dicionário que mapeia as palavras para o seu valor numérico\n",
        "# Labels: 0(Bad), 1(Good)\n",
        "\n",
        "# Escolher a review\n",
        "review = 4\n",
        "\n",
        "print(\"Numero de palavras: \" ,len(x_train[review]))\n",
        "print(x_train[review])\n",
        "\n",
        "tam = len(x_train[review])\n",
        "print('Label ', y_train[review])\n",
        "\n",
        "\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in x_train[review][:tam]])"
      ],
      "metadata": {
        "id": "NifJioNQx7pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cortar as reviews para um determinado tamanho máximo\n",
        "# Por omissão, o corte é feito no início e são mantidas apenas as maxlen palavras finais \n",
        "\n",
        "# O classificador terá que efetuar previsões tendo em consideração apenas as ultimas palavras da review\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "\n",
        "maxlen = 20\n",
        "\n",
        "x_trainP = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_testP = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "qmwmeHRp1f2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar algumas reviews\n",
        "# O sentido positivo/negativo da review é evidente neste excerto de texto?\n",
        "\n",
        "# Escolher a review\n",
        "review = 10\n",
        "tam = len(x_trainP[review])\n",
        "\n",
        "print('Label ', y_train[review])\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in x_trainP[review][:tam]])\n"
      ],
      "metadata": {
        "id": "zF6vMQWF1p6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma rede baseline que processe os inputs de forma simples\n",
        "\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen, 1])\n",
        "x = keras.layers.Flatten()(inputs)\n",
        "\n",
        "# Completar o modelo com as camadas escondidas\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "model_1 = keras.Model(inputs, output)"
      ],
      "metadata": {
        "id": "Vri07jJvZLEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detalhes\n",
        "\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "uGIlt887aLpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar e treinar o modelo 1\n",
        "\n",
        "model_1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model_1.fit(x_trainP, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "6UY-woXDakh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho no conjunto de teste\n",
        "\n",
        "model_1.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "JxkGCz0zbu07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar uma camada para embedding. O treino do embedding será feito pelo classificador\n",
        "# https://www.tensorflow.org/tutorials/text/word_embeddings\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "output_emb = 8\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen])\n",
        "emb = keras.layers.Embedding(max_features,  output_emb)(inputs)\n",
        "x = keras.layers.Flatten()(emb)\n",
        "\n",
        "# Completar o modelo com as camadas escondidas\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_2 = keras.Model(inputs, output)"
      ],
      "metadata": {
        "id": "saGCMKkzb6k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "T7L3dwDpcbUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model_2.fit(x_trainP, y_train,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "E67320VSc5QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho no conjunto de teste\n",
        "\n",
        "model_2.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "5_KYJQ4Xfdre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substituir os nós da rede neuronal das camadas escondidas por células LSTM\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# A saída recebe as contribuições pesadas \n",
        "\n",
        "output_emb = 8\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen])\n",
        "emb = keras.layers.Embedding(max_features,  output_emb)(inputs)\n",
        "\n",
        "# Completar o modelo com as camadas escondidas\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_3 = keras.Model(inputs, output)\n",
        "\n"
      ],
      "metadata": {
        "id": "X0hy1MlhfkpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.summary()"
      ],
      "metadata": {
        "id": "Ydqqe7E2gIm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model_3.fit(x_trainP, y_train,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.2)"
      ],
      "metadata": {
        "id": "Aa82uQBfgMvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho no conjunto de teste\n",
        "\n",
        "model_3.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "x9geOYkAgkz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizar um embedding pré-treinado\n",
        "# Neste exemplo será usado o embedding GloVe: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "# Guardar o ficheiro com os embedding na diretoria corrente ('glove.6B.50d.txt') \n",
        "# O ficheiro está disponível no Moodle\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "8lo6Dw_cgu7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma matriz onde será colocado o embedding\n",
        "# Tem max_words linhas e 50 colunas (a dimensão do embedding que vamos usar)\n",
        "\n",
        "embedding_dim = 50\n",
        "max_words = max_features\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # matriz com zeros\n",
        "\n",
        "# Preencher a matriz com os dados do embedding pré-treinado\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "GYHVp4bfB8gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar um modelo igual ao anterior, alterando apenas a dimensão do embedding\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen])\n",
        "emb = keras.layers.Embedding(max_words, embedding_dim)(inputs)\n",
        "\n",
        "# Completar o modelo com as camadas escondidas\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_4 = keras.Model(inputs, output)\n",
        "\n",
        "\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "XoYlkx0FCA6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colocar os valores do embedding pré-treinado na camada de embedding\n",
        "# Congelar estes pesos para que não se alterem durante o treino\n",
        "\n",
        "model_4.layers[1].set_weights([embedding_matrix])\n",
        "model_4.layers[1].trainable = False"
      ],
      "metadata": {
        "id": "qV11i9IqCDRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_4.fit(x_trainP, y_train,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.2)"
      ],
      "metadata": {
        "id": "AQ16eVDWCJf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "A5FdiW_wCLrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementar e testar possíveis melhorias\n",
        "\n",
        "# Variar alguns híper-parâmetros \n",
        "\n"
      ],
      "metadata": {
        "id": "MKbw2jJHC-98"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}